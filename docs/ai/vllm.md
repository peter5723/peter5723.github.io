# vllm


transformer

vllm 是现在非常常用的部署大模型的框架。

关键 KV-cache

paged-attention 